{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-28T15:41:55.165328Z","iopub.execute_input":"2023-09-28T15:41:55.165865Z","iopub.status.idle":"2023-09-28T15:41:55.175189Z","shell.execute_reply.started":"2023-09-28T15:41:55.165823Z","shell.execute_reply":"2023-09-28T15:41:55.173744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import library\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.182680Z","iopub.execute_input":"2023-09-28T15:41:55.183079Z","iopub.status.idle":"2023-09-28T15:41:55.199897Z","shell.execute_reply.started":"2023-09-28T15:41:55.183047Z","shell.execute_reply":"2023-09-28T15:41:55.197581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA : Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.202618Z","iopub.execute_input":"2023-09-28T15:41:55.203172Z","iopub.status.idle":"2023-09-28T15:41:55.235748Z","shell.execute_reply.started":"2023-09-28T15:41:55.203115Z","shell.execute_reply":"2023-09-28T15:41:55.234387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/titanic/test.csv')\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.238148Z","iopub.execute_input":"2023-09-28T15:41:55.238637Z","iopub.status.idle":"2023-09-28T15:41:55.262074Z","shell.execute_reply.started":"2023-09-28T15:41:55.238590Z","shell.execute_reply":"2023-09-28T15:41:55.260707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['Name', 'Ticket','Cabin', 'Embarked'], axis = 1, inplace = True)\ndf_test.drop(['Name', 'Ticket','Cabin', 'Embarked'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.263709Z","iopub.execute_input":"2023-09-28T15:41:55.264179Z","iopub.status.idle":"2023-09-28T15:41:55.273323Z","shell.execute_reply.started":"2023-09-28T15:41:55.264134Z","shell.execute_reply":"2023-09-28T15:41:55.271896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FEATURE ENGINEERING","metadata":{}},{"cell_type":"code","source":"df.Age = df.Age.fillna(df.Age.mean())\ndf_test.Age = df_test.Age.fillna(df_test.Age.mean())\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.277047Z","iopub.execute_input":"2023-09-28T15:41:55.277735Z","iopub.status.idle":"2023-09-28T15:41:55.302814Z","shell.execute_reply.started":"2023-09-28T15:41:55.277677Z","shell.execute_reply":"2023-09-28T15:41:55.301739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.321345Z","iopub.execute_input":"2023-09-28T15:41:55.322806Z","iopub.status.idle":"2023-09-28T15:41:55.335329Z","shell.execute_reply.started":"2023-09-28T15:41:55.322758Z","shell.execute_reply":"2023-09-28T15:41:55.334114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding new features : Family_size\ndf['Family_size'] = df['SibSp'] + df ['Parch'] + 1\ndf_test['Family_size'] = df_test['SibSp'] + df_test['Parch'] + 1\nnumerical_cols = ['Age', 'Pclass', 'Family_size']\n\n#Standardizing numerical data \nsc_X = StandardScaler()\nsc_X_train = sc_X.fit_transform(df[numerical_cols])\nsc_X_test = sc_X.fit_transform(df_test[numerical_cols])\n#Convert to table format - StandardScaler \nsc_X_train = pd.DataFrame(data=sc_X_train, columns=[\"Age\", \"Pclass\",\"Family_size\"])\nsc_X_test = pd.DataFrame(data=sc_X_test, columns=[\"Age\", \"Pclass\",\"Family_size\"])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.346261Z","iopub.execute_input":"2023-09-28T15:41:55.347539Z","iopub.status.idle":"2023-09-28T15:41:55.367897Z","shell.execute_reply.started":"2023-09-28T15:41:55.347493Z","shell.execute_reply":"2023-09-28T15:41:55.366325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['SibSp', 'Parch'], axis = 1)\ndf.replace(['male', 'female'], [-1,1], inplace = True)\ndf_test.drop(['SibSp', 'Parch'], axis = 1)\ndf_test.replace(['male', 'female'], [-1,1], inplace = True)\n\nX = pd.concat([df.Sex, sc_X_train], axis=1)\ny = df.Survived\n\nX_test = pd.concat([df_test.Sex, sc_X_test], axis=1)\ny_pred = []\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 420)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.370080Z","iopub.execute_input":"2023-09-28T15:41:55.372388Z","iopub.status.idle":"2023-09-28T15:41:55.397279Z","shell.execute_reply.started":"2023-09-28T15:41:55.372334Z","shell.execute_reply":"2023-09-28T15:41:55.396065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building model using Deep Learning with 3 layers","metadata":{}},{"cell_type":"code","source":"#Dropout and BatchNormalization to avoid overfitting\nmodel = keras.Sequential([\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1, activation = 'sigmoid'),\n])\n    \nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=25,\n    min_delta=0.01,\n    restore_best_weights=True,\n)\n\n#fitting the model into training dataset\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size= 32,\n    epochs=100,\n    callbacks=[early_stopping],\n    verbose=0, # hide the output because we have so many epochs\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:41:55.398883Z","iopub.execute_input":"2023-09-28T15:41:55.399533Z","iopub.status.idle":"2023-09-28T15:42:01.764625Z","shell.execute_reply.started":"2023-09-28T15:41:55.399487Z","shell.execute_reply":"2023-09-28T15:42:01.763703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model Evaluation\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:42:01.766242Z","iopub.execute_input":"2023-09-28T15:42:01.766748Z","iopub.status.idle":"2023-09-28T15:42:02.340266Z","shell.execute_reply.started":"2023-09-28T15:42:01.766718Z","shell.execute_reply":"2023-09-28T15:42:02.339358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Input the model into test dataset\ny_pred = model.predict(X_test).argmax(axis = 1)\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': y_pred})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T15:42:02.341352Z","iopub.execute_input":"2023-09-28T15:42:02.342389Z","iopub.status.idle":"2023-09-28T15:42:02.530564Z","shell.execute_reply.started":"2023-09-28T15:42:02.342342Z","shell.execute_reply":"2023-09-28T15:42:02.529376Z"},"trusted":true},"execution_count":null,"outputs":[]}]}